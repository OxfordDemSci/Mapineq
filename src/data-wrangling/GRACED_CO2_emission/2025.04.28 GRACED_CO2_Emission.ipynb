{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7dfae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import tarfile\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from osgeo import gdal, osr\n",
    "from tqdm import tqdm\n",
    "import rasterio\n",
    "from rasterstats import zonal_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac91178-1833-4969-8b52-76b70ba4c904",
   "metadata": {},
   "source": [
    "# 1. Uncompress and convert nc to tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b0c1555-6b71-461d-8ca4-0786cfa71142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 75/75 [06:33<00:00,  5.25s/it]\n"
     ]
    }
   ],
   "source": [
    "# Unzip all gz files\n",
    "read_folder = r'C:\\1-Data\\GRACED'\n",
    "save_folder = r'C:\\1-Data\\GRACED\\nc'\n",
    "\n",
    "for file in tqdm(os.listdir(read_folder)):\n",
    "    if file.endswith('gz'):\n",
    "        with tarfile.open(read_folder + '\\\\' + file) as tar:\n",
    "            tar.extractall(save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5faa461b-686d-406c-93d9-420ae198913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nc_to_geotiff(nc_file, save_path, day, dstSRS='EPSG:3035'):\n",
    "    \n",
    "    lat = nc_file.variables['latitude'][:]\n",
    "    lon = nc_file.variables['longitude'][:]\n",
    "    emission = np.asanyarray(nc_file.variables['emission'][day])\n",
    "\n",
    "    # get the spatial range of the netcdf\n",
    "    Lonmin, Latmax, Lonmax, Latmin = [lon.min(), lat.max(), lon.max(), lat.min()]\n",
    "\n",
    "    # calculate the resolution\n",
    "    Num_lat = len(lat)\n",
    "    Num_lon = len(lon)\n",
    "    Lat_res = (Latmax - Latmin) / (float(Num_lat) - 1)\n",
    "    Lon_res = (Lonmax - Lonmin) / (float(Num_lon) - 1)\n",
    "\n",
    "    # create the tif file and save it into the virtual file system in memory\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    out_tif = driver.Create('/vsimem/emission.tif', Num_lon, Num_lat, 1, gdal.GDT_Float32)\n",
    "\n",
    "    # set the spatial range of the tif file\n",
    "    geotransform = (Lonmin, Lon_res, 0.0, Latmax, 0.0, -Lat_res)\n",
    "    out_tif.SetGeoTransform(geotransform)\n",
    "\n",
    "    # set the projection system\n",
    "    prj = osr.SpatialReference()\n",
    "    prj.ImportFromEPSG(4326)\n",
    "    out_tif.SetProjection(prj.ExportToWkt())\n",
    "\n",
    "    # check is the data are flipped and correct the data if yes\n",
    "    if lat[0] <= lat[-1]: \n",
    "        emission = emission[::-1]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # write data into tif and close the file\n",
    "    out_tif.GetRasterBand(1).WriteArray(emission)\n",
    "    # transform the projection to 3035 and save\n",
    "    gdal.Warp(save_path, out_tif, srcSRS='EPSG:4326', dstSRS=dstSRS)\n",
    "    out_tif.FlushCache() \n",
    "    out_tif = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2d7cf6-eec1-4ee9-9df5-a2f3b7075ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert nc to tiff for nuts data extraction\n",
    "read_folder = r'C:\\1-Data\\GRACED\\nc'\n",
    "save_folder = r'C:\\1-Data\\GRACED\\tiff'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for netc in tqdm(os.listdir(read_folder)[:1]):\n",
    "        read_path = read_folder + r'/' + netc\n",
    "        nc_file = nc.Dataset(read_path)\n",
    "        days = nc_file.variables['nday'][:]\n",
    "        for day in days:\n",
    "            save_path = save_folder + r'/' + netc.split('_')[-2][-4:] + '_' + netc.split('_')[-1].split('.')[0][1:] + '_' + str(day) + '.tif'\n",
    "            nc_to_geotiff(nc_file, save_path, day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850d4ed3-2edb-4b91-9def-0e44d70b1a99",
   "metadata": {},
   "source": [
    "# 2. Zonal staitistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2d9cc8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 2251/2251 [20:08:22<00:00, 32.21s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2251/2251 [27:05:20<00:00, 43.32s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2251/2251 [27:21:32<00:00, 43.75s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2251/2251 [24:51:59<00:00, 39.77s/it]\n"
     ]
    }
   ],
   "source": [
    "nuts_folder = r'C:\\1-Data\\NUTS'\n",
    "read_folder = r'C:\\1-Data\\GRACED\\tiff'\n",
    "nuts_list = ['NUTS_RG_01M_2003_3035.shp', 'NUTS_RG_01M_2006_3035.shp', 'NUTS_RG_01M_2013_3035.shp', \n",
    "             'NUTS_RG_01M_2016_3035.shp', 'NUTS_RG_01M_2016_3035.shp', 'NUTS_RG_01M_2024_3035.shp']\n",
    "for nuts_file in nuts_list[2:]:\n",
    "    df_comb = None\n",
    "    nuts_file = nuts_folder + '\\\\' + nuts_file\n",
    "    nuts = gpd.read_file(nuts_file)\n",
    "    \n",
    "    for file in tqdm(os.listdir(read_folder)):\n",
    "        read_file = read_folder + '\\\\' + file\n",
    "       \n",
    "        # link the zonal statistics table to the boundary index\n",
    "        zs_temp = pd.DataFrame(zonal_stats(nuts_file, read_file, stats=['mean','sum']))\n",
    "        df_temp = pd.merge(nuts[['NUTS_ID']], zs_temp, left_index=True, right_index=True)\n",
    "        \n",
    "        # \"unstack\" the columns of 'mean' and 'sum' so that they are now in the same column of 'calculation'\n",
    "        mean_temp = df_temp[['NUTS_ID', 'mean']]\n",
    "        sum_temp = df_temp[['NUTS_ID', 'sum']]\n",
    "        mean_temp.columns = ['NUTS_ID', 'obsValue']\n",
    "        mean_temp.loc[:,'calculation'] = 'mean'\n",
    "        sum_temp.columns = ['NUTS_ID', 'obsValue']\n",
    "        sum_temp.loc[:,'calculation'] = 'sum'\n",
    "        df_daily = pd.concat([mean_temp, sum_temp])\n",
    "        df_daily.loc[:,'obsTime'] = file.split('.')[0]\n",
    "        df_comb = df_daily if df_comb is None else pd.concat([df_comb, df_daily])\n",
    "        \n",
    "    # save the table\n",
    "    geo_source = 'NUTS' + nuts_file.split('_')[-2]\n",
    "    df_comb.loc[:,'geo_source'] = geo_source\n",
    "    df_comb['id'] = df_comb.reset_index().index\n",
    "    df_comb = df_comb[['id'] + [i for i in df_comb.columns if i not in ['geo_source', 'id']]+['geo_source']]\n",
    "    \n",
    "    df_comb.to_csv(r'C:\\2-Case studies\\graced' + '\\\\' + geo_source + '.csv', index=False)    \n",
    "    # delete nuts and release the RAM\n",
    "    del nuts\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cad6d37-ab4c-4b7e-8734-6bd8fe6fcbd4",
   "metadata": {},
   "source": [
    "# 3. Aggregate daily emission to monthly and yearly levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d74b0f84-c577-46be-b7c5-451cf0a88ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 6/6 [33:50<00:00, 338.34s/it]\n"
     ]
    }
   ],
   "source": [
    "read_folder = r'C:\\2-Case studies\\graced\\daily'\n",
    "save_folder_m = r'C:\\2-Case studies\\graced\\monthly'\n",
    "save_folder_y = r'C:\\2-Case studies\\graced\\yearly'\n",
    "\n",
    "for file in tqdm(os.listdir(read_folder)):\n",
    "    df_daily = pd.read_csv(read_folder + '\\\\' + file)\n",
    "    df['month'] = df.apply(lambda x: x['geo'] + '_' + x['calculation'] + '_' + x['obsTime'][-7:], axis = 1)\n",
    "    df['year'] = df.apply(lambda x: x['geo'] + '_' + x['calculation'] + '_' + x['obsTime'][-4:], axis = 1)\n",
    "    \n",
    "    # aggregate daily data into monthly data \n",
    "    df_month = df.groupby('month')['obsValue'].sum().reset_index()\n",
    "    # reformat and save the table\n",
    "    df_month['geo'] = df_month['month'].apply(lambda x: x.split('_')[0])\n",
    "    df_month['calculation'] = df_month['month'].apply(lambda x: x.split('_')[1])\n",
    "    df_month['month'] = df_month['month'].apply(lambda x: x[-7:])\n",
    "    df_month['geo_source'] = file.split('.')[0]\n",
    "    df_month = df_month.rename(columns={'month':'obsTime'})\n",
    "    df_month['id'] = df_month.reset_index().index\n",
    "    df_month = df_month[['id'] + [i for i in df_month.columns if i not in ['geo_source', 'id']]+['geo_source']]\n",
    "    df_month.to_csv(save_folder_m + '\\\\' + file, index=False)\n",
    "\n",
    "    # aggregate daily data into yearly data\n",
    "    df_year = df.groupby('year')['obsValue'].sum().reset_index()\n",
    "    # reformat and save the table\n",
    "    df_year['geo'] = df_year['year'].apply(lambda x: x.split('_')[0])\n",
    "    df_year['calculation'] = df_year['year'].apply(lambda x: x.split('_')[1])\n",
    "    df_year['year'] = df_year['year'].apply(lambda x: x.split('_')[2])\n",
    "    df_year['geo_source'] = file.split('.')[0]\n",
    "    df_year = df_year.rename(columns={'year':'obsTime'})\n",
    "    df_year['id'] = df_year.reset_index().index\n",
    "    df_year = df_year[['id'] + [i for i in df_year.columns if i not in ['geo_source', 'id']]+['geo_source']]\n",
    "    df_year.to_csv(save_folder_y + '\\\\' + file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b4be016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_year_combine = None\n",
    "for file in os.listdir(save_folder_y):\n",
    "    df_year_temp = pd.read_csv(save_folder_y + '\\\\' + file)\n",
    "    df_year_combine = df_year_temp if df_year_combine is None else pd.concat([df_year_combine, df_year_temp])\n",
    "df_year_combine.to_csv(r'C:\\2-Case studies\\graced\\GRACED_CO2_yearly.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8f3ff781",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_month_combine = None\n",
    "for file in os.listdir(save_folder_m):\n",
    "    df_month_temp = pd.read_csv(save_folder_m + '\\\\' + file)\n",
    "    df_month_combine = df_month_temp if df_month_combine is None else pd.concat([df_month_combine, df_month_temp])\n",
    "df_month_combine.to_csv(r'C:\\2-Case studies\\graced\\GRACED_CO2_monthly.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60eeda88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_year_combine['freq'] = 'yearly'\n",
    "df_month_combine['freq'] = 'monthly'\n",
    "df_GRACED = pd.concat([df_year_combine, df_month_combine])\n",
    "df_GRACED['id'] = df_GRACED.reset_index().index\n",
    "df_GRACED = df_GRACED[['id'] + [i for i in df_GRACED.columns if i not in ['geo_source', 'id']]+['geo_source']]\n",
    "df_GRACED.to_csv(r'C:\\2-Case studies\\graced\\GRACED_final.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eb96f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17565c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d909ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7cbe91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
